{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projeto IV.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/EVhCQgs41zbLsqxMgN/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steffaneribeiro/Projeto_IV/blob/main/Projeto_IV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCzdGxjKUR5T"
      },
      "source": [
        "Links de referência:\n",
        "\n",
        "*   https://aws.amazon.com/pt/blogs/aws-brasil/criando-um-workflow-de-rotulamento-treinamento-e-deploy-de-machine-learning-utilizando-o-amazon-sagemaker-pytorch-e-amazon-sagemaker-ground-truth/\n",
        "*   https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html\n",
        "*   https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\n",
        "\n",
        "Conjunto de dados utilizado:\n",
        "\n",
        "*   http://www.vision.caltech.edu/Image_Datasets/Caltech256/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6gIyPPOUcn_"
      },
      "source": [
        "# **Treinamento e deploy de machine learning utilizando o Amazon SageMaker e PyTorch**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Meu projeto consistiu em treinar um modelo para classificar diferentes imagens de copos. Estas imagens foram selecionadas a partir do dataset público mais de 30 mil imagens rotuladas em 256 categorias. A partir deste dataset, trabalhei com uma amostra das seguintes 4 classes:\n",
        "\n",
        "*   Beer Mug (caneca de cerveja);\n",
        "*   Coffee Mug (caneca de café);\n",
        "*   Teapot (bule de chá);\n",
        "*   Wine Bottle (garrafa de vinho);\n",
        "\n",
        "Selecionei 20 imagens para treinamento e 5 para teste.\n",
        "\n",
        "Para começar, segui os seguintes passos:\n",
        "\n",
        "1.   Criei um bucket no Amazon S3, chamado piiv;\n",
        "2.   Em seguida, selecione vinte imagens aleatórias do dataset citado anteriormente, considerando as quatro categorias mencionadas, e fiz o upload dessas imagens para o S3, utilizando dois diretórios, \"train\" e \"test\".\n",
        "3.   Seguindo agora para o Amazon SageMaker, iniciei pelo treinamento do modelo. Utilizaando uma técnica de Machine Learning chamada [Transfer Learning](https://cs231n.github.io/transfer-learning/), fiz o upload do script transfer_learning.py com o código completo empregando a técnica de transfer learning e o framework PyTorch. Esse script foi o ponto de entrada para o treinamento e deploy do meu modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfrRhqtxXhGt"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms, models\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "def _get_data_loader(batch_size, training_dir, is_distributed, **kwargs):\n",
        "    logger.info(\"Get dataset into data_loader\")\n",
        "    \n",
        "    data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    }\n",
        "\n",
        "    data_dir = training_dir\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'test']}\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) if is_distributed else None\n",
        "    \n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                                shuffle=train_sampler is None, \n",
        "                                                sampler=train_sampler, **kwargs)\n",
        "                  for x in ['train', 'test']}\n",
        "    \n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "\n",
        "    return dataloaders, dataset_sizes\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    try:\n",
        "        logger.info('model_fn')\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n",
        "            ckpt = torch.load(f, map_location='cpu')\n",
        "        optimizer = ckpt['optimizer']\n",
        "        epoch = ckpt['epoch']\n",
        "        model = ckpt['model']\n",
        "        load_dict = OrderedDict()\n",
        "        for k, v in model.items():\n",
        "            if k.startswith('module.'):\n",
        "                k_ = k.replace('module.', '')\n",
        "                load_dict[k_] = v\n",
        "            else:\n",
        "                load_dict[k] = v\n",
        "        \n",
        "        model = models.resnet18(pretrained=False)\n",
        "        num_ftrs = model.fc.in_features\n",
        "        \n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 4), \n",
        "            nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        "        )\n",
        "        \n",
        "        model.load_state_dict(load_dict)\n",
        "        return model.to(device)\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        raise\n",
        "\n",
        "def save_model(model, optimizer, epoch, model_dir):\n",
        "    logger.info(\"Saving the model.\")\n",
        "    path = os.path.join(model_dir, 'model.pth')\n",
        "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
        "    torch.save(\n",
        "        {\n",
        "            \"model\" : model.state_dict(), \n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch\n",
        "        },\n",
        "        path)\n",
        "\n",
        "def train_model(dataloaders, dataset_sizes, device, model, criterion, optimizer, \n",
        "                scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def train(args):\n",
        "    is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
        "    logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
        "    use_cuda = args.num_gpus > 0\n",
        "    logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if is_distributed:\n",
        "        # Initialize the distributed environment.\n",
        "        world_size = len(args.hosts)\n",
        "        os.environ['WORLD_SIZE'] = str(world_size)\n",
        "        host_rank = args.hosts.index(args.current_host)\n",
        "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
        "        logger.info('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n",
        "            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
        "            dist.get_rank(), args.num_gpus))\n",
        "\n",
        "    # set the seed for generating random numbers\n",
        "    torch.manual_seed(args.seed)\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    dataloaders, dataset_sizes = _get_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
        "\n",
        "    model_ft = models.resnet18(pretrained=True)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    \n",
        "    # Change the final layer of ResNet18 Model for Transfer Learning\n",
        "    #model_ft.fc = nn.Linear(num_ftrs, 4)\n",
        "    model_ft.fc = nn.Sequential(\n",
        "        nn.Linear(num_ftrs, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(256, 4), \n",
        "        nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        "    )\n",
        "\n",
        "    model_ft = model_ft.to(device)\n",
        "    if is_distributed and use_cuda:\n",
        "        # multi-machine multi-gpu case\n",
        "        model_ft = torch.nn.parallel.DistributedDataParallel(model_ft)\n",
        "    else:\n",
        "        # single-machine multi-gpu case or single-machine or multi-machine cpu case\n",
        "        model_ft = torch.nn.DataParallel(model_ft)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Training\n",
        "    model_ft = train_model(dataloaders, dataset_sizes, device, model_ft, criterion, \n",
        "                            optimizer_ft, exp_lr_scheduler, args.epochs)\n",
        "\n",
        "    # Save Model\n",
        "    save_model(model_ft, optimizer_ft, args.epochs, args.model_dir)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Data and model checkpoints directories\n",
        "    parser.add_argument('--batch-size', type=int, default=4, metavar='N',\n",
        "                        help='input batch size for training (default: 4)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "                        help='number of epochs to train (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
        "                        help='learning rate (default: 0.001)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
        "                        help='SGD momentum (default: 0.9)')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--backend', type=str, default=None,\n",
        "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
        "\n",
        "    # Container environment\n",
        "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n",
        "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n",
        "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
        "    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
        "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
        "\n",
        "    train(parser.parse_args())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUwN5Vwua0g8"
      },
      "source": [
        "4. Logo em segida, criei a instância de notebook  utilizando o ***kernel conda_pytorch_p36***\n",
        "\n",
        "OBS: Como o propósito deste projeto para teste de alguns dos serviços de aprendizagem de máquina disponíveis na plataforma AWS e praticar o conhecimento adquirido no curso AWS Academy Machine Learning Foundations, pode ser utilizada uma instância do tipo ***ml.t3.medium***.\n",
        "\n",
        "\n",
        "> Num primeiro trecho desse notebook importei algumas dependências que irei utilizar e definir o caminho para o bucket\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWsWDFuJdBBw"
      },
      "source": [
        "import os, json\n",
        "import pandas as pd\n",
        "import fnmatch\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "import sagemaker\n",
        "\n",
        "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "s3 = boto3.resource('s3')\n",
        "\n",
        "# Destination Bucket\n",
        "bucket = 'piiv'\n",
        "role = sagemaker.get_execution_role()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaUMr-2yd0pM"
      },
      "source": [
        "\n",
        "\n",
        "> No mesmo notebook, criei um estimator do SageMaker, que possui as instruções necessárias para o processo de treinamento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFk0A2vId_V7"
      },
      "source": [
        "from sagemaker.pytorch import PyTorch\n",
        "\n",
        "# Criando o Estimator\n",
        "pytorch_estimator = PyTorch('transfer_learning.py',\n",
        "                            role=role,\n",
        "                            instance_type='ml.m5.large',\n",
        "                            instance_count=1,\n",
        "                            framework_version='1.5.0',\n",
        "                            py_version='py3',\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHq1TXsDeBgE"
      },
      "source": [
        "\n",
        "\n",
        "> Adicionando mais um trecho de código, agora iremos pegar o caminho para as imagens no S3 e passa como parâmetro para o processo de treinamento do SageMaker. Durante este treinamento o SageMaker irá lançar as instâncias necessárias (definidas no passo anterior), executar o treinamento, salvar o modelo treinado no S3 e encerrar as instâncias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guE4G3zsjoOW"
      },
      "source": [
        "bucket_uri = 's3://' + bucket\n",
        "pytorch_estimator.fit({'training': bucket_uri})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Aho09ejj4VQ"
      },
      "source": [
        "> Após a conclusão do processo de treinamento, é possível fazer o deploy do modelo em uma instância de inferência do SageMaker. Esta instância é otimizada e preparada com as dependências necessárias para reduzir a necessidade de gerenciamento de infraestrutura.\n",
        "\n",
        "> Através do seguinte comando é possível criar uma instância de inferência para testar o modelo:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avkIfmb4j9zR"
      },
      "source": [
        "predictor = pytorch_estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXQMfCw5kA4a"
      },
      "source": [
        "#Testando o Modelo\n",
        "\n",
        "Para testar, utilizei o script abaixo, que irá utilizar uma imagem da pasta de validation, composta por imagens do dataset original, mas que não foram utilizadas no processo do treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz6qanhxkhsh"
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "loader = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])        \n",
        "\n",
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).float()\n",
        "    image = image.unsqueeze(0) \n",
        "    return image\n",
        "\n",
        "s3 = boto3.resource('s3')\n",
        "bucket = s3.Bucket('piiv')\n",
        "object = bucket.Object('010_0009.jpg') #key\n",
        "response = object.get()\n",
        "img = response['Body']\n",
        "\n",
        "image = image_loader(img)\n",
        "\n",
        "objects_category = ['01-beer-mug','02-coffee-mug','03-teapot','04-wine-bottle']\n",
        "\n",
        "response = predictor.predict(image)\n",
        "output = torch.exp(torch.tensor(response))\n",
        "index = np.argmax(output)\n",
        "print(\"Result --> label: \" + objects_category[index] + \" | probability: \" + str(output[0][index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RA0bx_PkriN"
      },
      "source": [
        "O resultado é o output abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S5WTqLXkvi_"
      },
      "source": [
        "Result --> label: 01-beer-mug | probability: tensor(0.6802, dtype=torch.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvo0XL3Tdfwj"
      },
      "source": [
        "\n",
        "\n",
        "> \n",
        "\n"
      ]
    }
  ]
}